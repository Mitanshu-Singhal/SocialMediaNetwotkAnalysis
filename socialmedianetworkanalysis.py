# -*- coding: utf-8 -*-
"""SocialMediaNetworkAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kL0UKJhqHKOPom1Ws0VDlnRxtCL2p8XS
"""

#importing packages
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

ads_df = pd.read_csv('/content/Social_Network_Ads.csv')
ads_df.head()

from google.colab import drive
drive.mount('/content/drive')

#calculating total number of rows and columns
ads_df.shape

"""# **EDA**"""

#checking null values in the dataset
ads_df.isnull().sum()

"""**None of the columns has the null values**"""

#Checking any duplicated values in the dataset
ads_df.duplicated().sum()

"""**None of the rows has duplicate values**"""

#Dropping unnecessary columns
#In our case 'User ID' is an unnecessary column
ads_df.drop(columns=['User ID'],inplace=True)

#checking if the particular column has been removed or not
ads_df.head()

#calculating the number of rows and columns to verify if column is reduced or not
ads_df.shape

#calculating total numbe of female and males in the data
ads_df.Gender.value_counts()

#plotting in a graph total the above count
ads_df.Gender.value_counts().plot(kind='barh') # kind = 'bar' represents to plot the bar graph and 'barh' reresents to plot the horizontal bar graph

"""**Number of females is greater than number of males and difference is not that big**"""

#Counting the age in a datasets
ads_df["Age"].value_counts()

#plotting the age count in a distribution graph
sns.displot(ads_df['Age'])

#describing age
ads_df['Age'].describe()

"""**Age is normally distributed around 37**"""

#Plotting Estimated Salary
sns.displot(ads_df['EstimatedSalary'])

#describing salary
ads_df["EstimatedSalary"].describe()

#finding the relation between age and EstimatedSalary
plt.scatter(x='Age',y='EstimatedSalary',data=ads_df)

"""**There is a Cluster formed in the range of age = 35 to 40 and EstimatedSalary = 50000 to 80000 in the above Graph**

**As we can see there is no corelation between age and estimated salary**
"""

#finding the relation between age and Gender
sns.boxplot(x='Age',y='Gender',data=ads_df)

"""**Females of older age are more**"""

#finding the relation between Salary and Gender
sns.boxplot(x='EstimatedSalary',y='Gender',data=ads_df)

"""**Estimated Salary of males and females is approximately same**"""

#finding the relation between age and Gender based on purchased
sns.boxplot(x='Age',y='Gender',data=ads_df, hue='Purchased')

"""**It contains 1 outlier**

**It can be seen that older age of males and females are more probable to purchase than younger ones**
"""

#finding relation between Estimated Salary and Gender based on purchased
sns.boxplot(x='Gender',y='EstimatedSalary',data=ads_df, hue='Purchased')

"""**It contains 2 Outliers**

**EstimatedSalary Mean and median of females are greater than men and people with high salary most probable to purchase**
"""

#Multivariate Analysis
sns.pairplot(ads_df, hue="Purchased")

"""# Modelling"""

#importing packages for modelling and preprocessing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, mean_squared_error
from sklearn.naive_bayes import GaussianNB

#Getting the information about ads_df dataframe
ads_df.info()

#feature variable or independent variable
x = ads_df.iloc[:,:-1]
x

#target variable
y = ads_df.iloc[:,-1]
y.head()

#checking the shape of x and y
print('The shape of feature variable x =',x.shape)
print('The shape of target variable y =',y.shape)

#Encoding Categorical data i.e. Gender
le = LabelEncoder()
x['Gender']=le.fit_transform(x.iloc[:,0])

#Getting the info after encoding Gender column
x.info()

x.head()

#splitting the dataset into train and test sets
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=42)

#checking the shape of x_train, y_train, x_test, y_test
print('The shape of x_train =',x_train.shape)
print('The shape of y_train =',y_train.shape)
print('The shape of x_test =',x_test.shape)
print('The shape of y_test =',y_test.shape)

x_train.tail()

x_test.tail()

#Feature Scaling
sns.kdeplot(ads_df['Age'])

#Feature Scaling
sns.kdeplot(ads_df['EstimatedSalary'])

sns.kdeplot(ads_df['Age'])
sns.kdeplot(ads_df['EstimatedSalary'])

x_train.describe()

"""**As we can see there is a huge difference between minimum and maximum values**"""

x_test.describe()

"""**Normalising the data**"""

#Feature Scaling -> method to normalise the data
ss = StandardScaler()
x_train = ss.fit_transform(x_train)
x_test = ss.transform(x_test)

x_train = pd.DataFrame(x_train)
x_train.describe()

x_test = pd.DataFrame(x_test)
x_test.describe()

"""# **Logistic Regression**"""

#Building the model
lr_model = LogisticRegression()
#Training the model
lr_model.fit(x_train,y_train)

#Calculating the training score of the model
lr_training_score = lr_model.score(x_train, y_train)*100
print('Training Score of Logistic Regression Model =',lr_training_score,'%')

"""**Training Score of Logistic Regression Model = 81.5625%**"""

#Testing the model
y_pred_lr = lr_model.predict(x_test)
y_pred_lr

#Calculating the testing score
lr_testing_score = lr_model.score(x_test,y_test)*100
print('Testing Score of Logistic Regression =',lr_testing_score,'%')

"""**Testing Score of Logistic Regression = 88.75%**"""

#Calculating the Accuracy of the model
lr_accuracy = accuracy_score(y_test,y_pred_lr)*100
print('The accuracy of Logistic Regression =',lr_accuracy,'%')

"""**The accuracy of Losgistic Regression = 88.75%**"""

#Calculating Error in the model
lr_mse = mean_squared_error(y_test,y_pred_lr)*100
print('Error in the Logistic Regression Model =',lr_mse,'%')

"""**Error in the Logistic Regression Model = 11.25 %**

# **SVM MODEL**
"""

#Building the model
svc_model = SVC()
#Training the model
svc_model.fit(x_train,y_train)

#Calculating the training score of the model
svc_training_score = svc_model.score(x_train, y_train)*100
print('Training Score of SVC Model =',svc_training_score,'%')

"""**Training Score of SVC Model = 91.25%**"""

#Testing the model
y_pred_svc = svc_model.predict(x_test)
y_pred_svc

#Calculating the testing score
svc_testing_score = svc_model.score(x_test, y_test)*100
print('Testing Score of SVC Model =',svc_testing_score,'%')

"""**Testing Score of SVC Model = 92.5%**"""

#Calculating Overall Accuracy score
svc_acc_score = accuracy_score(y_test,y_pred_svc)*100
print('Accuracy of the SVC model =',svc_acc_score,'%')

"""**Accuracy of the SVC model = 92.5%**"""

#Calculating Mean Squared Error
svc_mse = mean_squared_error(y_test,y_pred_svc)*100
print('Error in the SVC model =',svc_mse,'%')

"""**Error in the SVC model = 7.5%**

# **KNN MODEL**
"""

#Building the KNN model
knn_model = KNeighborsClassifier()
#Training the model
knn_model.fit(x_train,y_train)

#Calculating the training score of the model
knn_training_score = knn_model.score(x_train, y_train)*100
print('Training Score of KNN Model =',knn_training_score,'%')

"""**Training Score of KNN Model = 91.25%**"""

#testing the model
y_pred_knn = knn_model.predict(x_test)
y_pred_knn

"""**Predict method returns the array of predicted output**"""

#Calculating the testing score
knn_testing_score = knn_model.score(x_test, y_test)*100
print('Testing Score of KNN Model =',knn_testing_score,'%')

"""**Testing Score of KNN Model = 92.5%**"""

#Calculating Overall Accuracy score
knn_acc_score = accuracy_score(y_test,y_pred_knn)*100
print('Accuracy of the KNN model =',knn_acc_score,'%')

"""**Accuracy of the KNN model = 92.5%**"""

#Calculating Mean Squared Error
knn_mse = mean_squared_error(y_test,y_pred_knn)*100
print('Error in the KNN model =',knn_mse,'%')

"""**Error in the KNN model = 7.5%**

# **Naive Bayes**
"""

#Building the model
gnb= GaussianNB()
#training the model
gnb.fit(x_train,y_train)

#calculating the training score of Naive Bayes
gnb_training_score = gnb.score(x_train,y_train)*100
print('Training score for Naive Bayes =',gnb_training_score,'%')

"""**Training score for Naive Bayes = 86.25%**"""

#calculating the testing score of Naive Bayes
gnb_testing_score = gnb.score(x_test,y_test)*100
print('Testing score for Naive Bayes =',gnb_testing_score,'%')

"""**Testing score for Naive Bayes = 93.75%**"""

#Testing the model
y_pred_gnb = gnb.predict(x_test)
y_pred_gnb

#Calculating the Acuracy of the model
gnb_accuracy = accuracy_score(y_test,y_pred_gnb)*100
print('Accuracy of the Naive Bayes =',gnb_accuracy,'%')

"""**Accuracy of the Naive Bayes = 93.75%**"""

gnb_mse = mean_squared_error(y_test,y_pred_gnb)*100
print('Error in Naive Bayes Model =',gnb_mse,'%')

"""**Error in Naive Bayes Model = 6.25 %**

# **Comparison of each model used above**
"""

models = ['Logistic Regression', 'SVM', 'KNN','Naive Bayes']
training_scores = [lr_training_score, svc_training_score, knn_training_score, gnb_training_score]
testing_scores = [lr_testing_score, svc_testing_score, knn_testing_score, gnb_testing_score]
accuracies = [lr_accuracy, svc_acc_score, knn_acc_score, gnb_accuracy]
errors = [lr_mse, svc_mse, knn_mse, gnb_mse]
data = np.array([training_scores,testing_scores,accuracies,errors])

#Graph for training score of each model

barWidth = 0.17
x = np.arange(len(models))

# define figure and axis objects
fig, ax = plt.subplots(figsize=(20, 8))

# Set position of bar on X axis
br1 = np.arange(len(training_scores))
br2 = [x + barWidth for x in br1]
br3 = [x + barWidth for x in br2]
br4 = [x + barWidth for x in br3]

ax.bar(br1, training_scores, width = barWidth,color = 'r', label = 'Training Score')
ax.bar(br2, testing_scores, width = barWidth, color = 'g', label = 'Testing Score')
ax.bar(br3, accuracies, width = barWidth, color = 'b', label = 'Accuracy')
ax.bar(br4, errors, width = barWidth, color = 'm', label = 'Error')

# add text annotations to each bar
for i, j in enumerate(data):
   for x_val, y_val in zip(x, j):
      ax.annotate(str(y_val), xy=((x_val+i*barWidth-barWidth/2)+0.08, y_val), ha='center', va='bottom')

ax.set_xlabel('Models')
ax.set_xticks([r + barWidth for r in range(len(training_scores))],models)
ax.set_yticks([0,10,20,30,40,50,60,70,80,90,100])
ax.set_ylabel('Scores')
ax.set_title('Comparison Of Each Model Used')
ax.legend()
plt.show()